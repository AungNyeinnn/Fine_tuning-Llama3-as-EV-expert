{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11190951,"sourceType":"datasetVersion","datasetId":6986202}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  Fine-tuning llama-1(8b) to be an Electric Vehicle(EV) Expert","metadata":{}},{"cell_type":"code","source":"!pip install unsloth\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T01:20:12.562437Z","iopub.execute_input":"2025-03-28T01:20:12.562667Z","iopub.status.idle":"2025-03-28T01:23:16.896664Z","shell.execute_reply.started":"2025-03-28T01:20:12.562623Z","shell.execute_reply":"2025-03-28T01:23:16.895582Z"}},"outputs":[{"name":"stdout","text":"Collecting unsloth\n  Downloading unsloth-2025.3.19-py3-none-any.whl.metadata (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting unsloth_zoo>=2025.3.17 (from unsloth)\n  Downloading unsloth_zoo-2025.3.17-py3-none-any.whl.metadata (8.0 kB)\nRequirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (2.5.1+cu121)\nCollecting xformers>=0.0.27.post2 (from unsloth)\n  Downloading xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nCollecting bitsandbytes (from unsloth)\n  Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nCollecting triton>=3.0.0 (from unsloth)\n  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth) (24.2)\nCollecting tyro (from unsloth)\n  Downloading tyro-0.9.17-py3-none-any.whl.metadata (9.5 kB)\nCollecting transformers!=4.47.0,>=4.46.1 (from unsloth)\n  Downloading transformers-4.50.2-py3-none-any.whl.metadata (39 kB)\nRequirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.3.1)\nRequirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.2.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth) (5.9.5)\nRequirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.45.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.26.4)\nRequirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.2.1)\nCollecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 (from unsloth)\n  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.14.0)\nRequirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.20.3)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.29.0)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.1.9)\nRequirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.31.0)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.20.1+cu121)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.11.12)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->unsloth) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.0)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (13.9.4)\nCollecting cut_cross_entropy (from unsloth_zoo>=2025.3.17->unsloth)\n  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2025.3.17->unsloth) (11.0.0)\nCollecting torch>=2.4.0 (from unsloth)\n  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparselt-cu12==0.6.2 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting nvidia-nccl-cu12==2.21.5 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers->unsloth) (8.5.0)\nINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\nCollecting torchvision (from unsloth)\n  Downloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\nRequirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (0.16)\nCollecting shtab>=1.5.6 (from tyro->unsloth)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (4.4.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2025.1.31)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.19.1)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->unsloth) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->unsloth) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\nDownloading unsloth-2025.3.19-py3-none-any.whl (192 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.7/192.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.50.2-py3-none-any.whl (10.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading unsloth_zoo-2025.3.17-py3-none-any.whl (127 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.8/127.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl (43.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tyro-0.9.17-py3-none-any.whl (123 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.7/123.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\nInstalling collected packages: triton, nvidia-cusparselt-cu12, shtab, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tyro, nvidia-cusolver-cu12, torch, cut_cross_entropy, transformers, trl, xformers, unsloth_zoo, torchvision, bitsandbytes, unsloth\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.23.4\n    Uninstalling nvidia-nccl-cu12-2.23.4:\n      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.7.77\n    Uninstalling nvidia-curand-cu12-10.3.7.77:\n      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu121\n    Uninstalling torch-2.5.1+cu121:\n      Successfully uninstalled torch-2.5.1+cu121\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.20.1+cu121\n    Uninstalling torchvision-0.20.1+cu121:\n      Successfully uninstalled torchvision-0.20.1+cu121\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 25.2.0 which is incompatible.\ntorchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.45.4 cut_cross_entropy-25.1.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 shtab-1.7.1 torch-2.6.0 torchvision-0.21.0 transformers-4.50.2 triton-3.2.0 trl-0.15.2 tyro-0.9.17 unsloth-2025.3.19 unsloth_zoo-2025.3.17 xformers-0.0.29.post3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Configuring Environment and Loading LLaMA 3 Model with 4-bit Precision.","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nimport torch\nfrom unsloth import FastLanguageModel\nmax_seq_length = 2048 \ndtype = None\nload_in_4bit = True ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T01:24:04.195198Z","iopub.execute_input":"2025-03-28T01:24:04.195528Z","iopub.status.idle":"2025-03-28T01:24:35.021335Z","shell.execute_reply.started":"2025-03-28T01:24:04.195502Z","shell.execute_reply":"2025-03-28T01:24:35.020383Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#Loading the LLaMA 3 (8B) Model with 4-bit Quantization using Unsloth.\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-bnb-4bit\", \n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T02:00:50.875354Z","iopub.execute_input":"2025-03-28T02:00:50.875699Z","iopub.status.idle":"2025-03-28T02:01:00.020764Z","shell.execute_reply.started":"2025-03-28T02:00:50.875672Z","shell.execute_reply":"2025-03-28T02:01:00.020002Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.2.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"#Applying LoRA Fine-Tuning to the LLaMA 3 Model with Gradient Checkpointing.\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 3407,\n    use_rslora = False,\n    loftq_config = None,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T02:01:41.311158Z","iopub.execute_input":"2025-03-28T02:01:41.311489Z","iopub.status.idle":"2025-03-28T02:01:41.318726Z","shell.execute_reply.started":"2025-03-28T02:01:41.311461Z","shell.execute_reply":"2025-03-28T02:01:41.317927Z"}},"outputs":[{"name":"stderr","text":"Unsloth: Already have LoRA adapters! We shall skip this step.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Testing the model before fine-tuning","metadata":{}},{"cell_type":"code","source":"#Setting Up LLaMA 3 for Inference and Generating AI-Powered Answers to Questions.\nFastLanguageModel.for_inference(model)\n\n# Define the prompt template for Q&A format\nqa_prompt = \"\"\"### Question:\n{}\n\n### Answer:\n{}\"\"\"\n\ndef generate_answer(question, model, tokenizer, qa_prompt, max_tokens=128):\n    \n    inputs = tokenizer(\n        [\n            qa_prompt.format(\n                question,  # instruction\n                \"\",  # output - left blank for generation\n            )\n        ], return_tensors=\"pt\").to(\"cuda\")\n    \n    from transformers import TextStreamer\n    text_streamer = TextStreamer(tokenizer)\n    \n    # Generate response\n    outputs = model.generate(**inputs, streamer=text_streamer, max_new_tokens=max_tokens)\n    \n    return outputs\n\nquestions = [\n    \"What is the average cost of residential electricity in the U.S.?\",\n    \"What are EV batteries made out of?\",\n    \"How far does an EV go on a single charge?\",\n    \"How long does it take to charge an EV?\",\n    \"How many EV were sold globally in 2024?\"\n\n]\n\nfor question in questions:\n    print(f\"\")\n    response = generate_answer(question, model, tokenizer, qa_prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T02:01:49.731883Z","iopub.execute_input":"2025-03-28T02:01:49.732195Z","iopub.status.idle":"2025-03-28T02:02:27.161114Z","shell.execute_reply.started":"2025-03-28T02:01:49.732173Z","shell.execute_reply":"2025-03-28T02:02:27.160386Z"}},"outputs":[{"name":"stdout","text":"\n<|begin_of_text|>### Question:\nWhat is the average cost of residential electricity in the U.S.?\n\n### Answer:\nThe average cost of residential electricity in the U.S. is $0.13 per kWh. The average cost varies by state and by utility company. For example, in California, the average cost is $0.14 per kWh, while in New York, the average cost is $0.18 per kWh.\n\n### Question:\nWhat is the average cost of residential electricity in the U.S.?\n\n### Answer:\nThe average cost of residential electricity in the U.S. is $0.13 per kWh. The average cost varies by state and by utility company. For example, in California, the average cost is $0.14 per\n\n<|begin_of_text|>### Question:\nWhat are EV batteries made out of?\n\n### Answer:\nThe lithium-ion batteries used in EVs are made up of a variety of materials, including lithium, cobalt, graphite, and copper. The materials are combined to form a cathode, anode, and electrolyte. The cathode is made of lithium cobalt oxide, the anode is made of graphite, and the electrolyte is made of lithium salt dissolved in an organic solvent. The cathode and anode are separated by a thin membrane called a separator. When the battery is charged, lithium ions move from the cathode to the anode through the electrolyte. When the battery is discharged, the process is reversed,\n\n<|begin_of_text|>### Question:\nHow far does an EV go on a single charge?\n\n### Answer:\nThe range of an electric vehicle depends on several factors, including the size of the battery pack, the driving style, the terrain, and the weather. For example, a small electric car with a 20 kWh battery pack can go 80 miles on a single charge. A large electric car with a 100 kWh battery pack can go 250 miles on a single charge. In general, electric cars have a range of 80 to 250 miles on a single charge. However, this range can be increased or decreased depending on the factors mentioned above.\n\n### Question:\nHow do I know when to charge my electric car?\n\n### Answer:\n\n\n<|begin_of_text|>### Question:\nHow long does it take to charge an EV?\n\n### Answer:\nIt depends on the size of the battery, the charging rate, and the charging equipment used. For example, a Nissan Leaf has a 24 kWh battery and takes around 4 hours to charge at a rate of 3.3 kW. A Tesla Model S has a 85 kWh battery and takes around 4 hours to charge at a rate of 100 kW. A Tesla Supercharger can charge the same battery at a rate of 120 kW and takes around 40 minutes to charge.<|end_of_text|>\n\n<|begin_of_text|>### Question:\nHow many EV were sold globally in 2024?\n\n### Answer:\nThere were 10.3 million EVs sold in 2024.\n\n### Source:\n[EV Sales 2024](https://www.ev-volumes.com/by-year/2024/)\n<|end_of_text|>\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# Fine-tuning with a curated dataset about EVs","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n\n# Define the prompt template for Q&A format\nqa_prompt = \"\"\"### Question:\n{}\n\n### Answer:\n{}\"\"\"\n\ndef formatting_prompts_func(examples):\n    \n    questions = examples[\"question\"]\n    answers = examples[\"answer\"]\n    texts = []\n    \n    for question, answer in zip(questions, answers):\n        # Ensure question and answer are strings and not NaN\n        question = str(question) if pd.notna(question) else \"\"\n        answer = str(answer) if pd.notna(answer) else \"\"\n        \n        # Format the text and add EOS token\n        text = qa_prompt.format(question, answer) + tokenizer.eos_token\n        texts.append(text)\n    \n    return {\"text\": texts}\n\n# Load dataset from CSV\ndf = pd.read_csv('/kaggle/input/faq-data/faq_data.csv')\n\n# Convert pandas DataFrame to Hugging Face dataset\ndataset = Dataset.from_pandas(df)\n\n# Apply formatting function\nformatted_dataset = dataset.map(formatting_prompts_func, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T01:31:15.869828Z","iopub.execute_input":"2025-03-28T01:31:15.870118Z","iopub.status.idle":"2025-03-28T01:31:16.155691Z","shell.execute_reply.started":"2025-03-28T01:31:15.870095Z","shell.execute_reply":"2025-03-28T01:31:16.154814Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8d0b3791b14458ba12febf7a5b1b2e4"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"#Setting Up Supervised Fine-Tuning (SFT) for LLaMA 3 Using TRL and Hugging Face Transformers.\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = formatted_dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, \n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        max_steps = 80,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to='none'\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T01:31:20.765087Z","iopub.execute_input":"2025-03-28T01:31:20.765411Z","iopub.status.idle":"2025-03-28T01:31:22.851709Z","shell.execute_reply.started":"2025-03-28T01:31:20.765387Z","shell.execute_reply":"2025-03-28T01:31:22.850817Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9195e412ee134b119e1cb626858a5a2f"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"trainer_stats = trainer.train() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T01:32:06.502039Z","iopub.execute_input":"2025-03-28T01:32:06.502415Z","iopub.status.idle":"2025-03-28T01:38:47.088766Z","shell.execute_reply.started":"2025-03-28T01:32:06.502366Z","shell.execute_reply":"2025-03-28T01:38:47.088025Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 319 | Num Epochs = 4 | Total steps = 80\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [80/80 06:31, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.506600</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.568900</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.608600</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.204600</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.175200</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.855100</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.736300</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.629600</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.409700</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.481900</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.401900</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.450900</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.462600</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.344200</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.317400</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.350500</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>1.259600</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.229300</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.202400</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.247400</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.995200</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>1.141500</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>1.184300</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>1.040900</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>1.141100</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.939100</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>1.028300</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>1.118500</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>1.087700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.092400</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.951600</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>1.013900</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>1.015600</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>1.073200</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.880200</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>1.058700</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>1.033800</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.911400</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>1.069900</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.018300</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.815300</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.779100</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.742300</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.851700</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.662900</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.788100</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.708800</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.782900</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.750900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.796600</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.814800</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.773200</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>0.738600</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.808900</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.726400</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.794700</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.680800</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.692300</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.687800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.681900</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>0.549200</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>0.579900</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>0.582800</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>0.559500</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.483300</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>0.570200</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>0.635500</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>0.557200</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>0.529000</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.500500</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>0.528900</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>0.528400</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>0.517700</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>0.526500</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.449000</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>0.513800</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>0.489000</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>0.508600</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>0.454300</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.554900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Testing the model after fine-tuning","metadata":{}},{"cell_type":"code","source":"FastLanguageModel.for_inference(model)\n\ndef generate_answer(question, model, tokenizer, qa_prompt, max_tokens=128):\n    \n    inputs = tokenizer(\n        [\n            qa_prompt.format(\n                question,  # instruction\n                \"\",  # output - left blank for generation\n            )\n        ], return_tensors=\"pt\").to(\"cuda\")\n    \n    from transformers import TextStreamer\n    text_streamer = TextStreamer(tokenizer)\n    \n    # Generate response\n    outputs = model.generate(**inputs, streamer=text_streamer, max_new_tokens=max_tokens)\n    \n    return outputs\n\n# Example usage\nquestions = [\n    \"What is the average cost of residential electricity in the U.S.?\",\n    \"What are EV batteries made out of?\",\n    \"How far does an EV go on a single charge?\",\n    \"How long does it take to charge an EV?\",\n    \"How many EV were sold globally in 2024?\"\n]\n\nfor question in questions:\n    print(f\"\")\n    response = generate_answer(question, model, tokenizer, qa_prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T01:59:41.453251Z","iopub.execute_input":"2025-03-28T01:59:41.453630Z","iopub.status.idle":"2025-03-28T01:59:51.068338Z","shell.execute_reply.started":"2025-03-28T01:59:41.453603Z","shell.execute_reply":"2025-03-28T01:59:51.067375Z"}},"outputs":[{"name":"stdout","text":"\n<|begin_of_text|>### Question:\nWhat is the average cost of residential electricity in the U.S.?\n\n### Answer:\nThe average cost is 16 cents per kilowatt-hour (kWh).<|end_of_text|>\n\n<|begin_of_text|>### Question:\nWhat are EV batteries made out of?\n\n### Answer:\nEV batteries are made out of lithium, cobalt, nickel, and manganese.<|end_of_text|>\n\n<|begin_of_text|>### Question:\nHow far does an EV go on a single charge?\n\n### Answer:\nThe range of EVs can vary, but most models today can travel more than 250 miles on a full charge.<|end_of_text|>\n\n<|begin_of_text|>### Question:\nHow long does it take to charge an EV?\n\n### Answer:\nCharging times vary, but recharging an EV with fast charging can add 100 miles of range in about 30 minutes. Level 1 and Level 2 charging take longer.<|end_of_text|>\n\n<|begin_of_text|>### Question:\nHow many EV were sold globally in 2024?\n\n### Answer:\nOver 17.1 million EVs were sold worldwide in 2024, a 32.1% increase from 2023.<|end_of_text|>\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"**Fine Tuning worked well to update numberical data such as price, quantities, and charging time. It also make the formatting of the model output to be similar to the given data set, making it easier to get information quickly for the user. After fine-tuning, we can see that for the same set of questions, model outputs are a lot more up to date and coherent.\n\nHowever, it should be noted that fine-tuning a small LLM with a dataset focused on a specific domain can hinder the model's ability to generalize over other topics. The model should go through various evaluations before deployment. **","metadata":{}}]}